{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 12:55:51,530\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 12:56:11,426\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms import ppo\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.examples.policy.random_policy import RandomPolicy\n",
    "from rl.envs import OthelloEnv\n",
    "\n",
    "register_env(\"othello\", lambda _: OthelloEnv({}))\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    agent_id = int(agent_id[-1]) - 1\n",
    "    return \"agent_1\" if episode.episode_id % 2 == agent_id else \"agent_2\"\n",
    "\n",
    "config = ppo.PPOConfig().environment(\"othello\").framework(\"torch\").rollouts(num_rollout_workers=8)\n",
    "config = config.multi_agent(policies={\"agent_1\": PolicySpec(), \"agent_2\": PolicySpec()}, policy_mapping_fn=policy_mapping_fn, policies_to_train=[\"agent_1\"])\n",
    "config = config.training(model={\"conv_filters\": [[32, [3, 3], 1], [64, [3, 3], 1]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-10 13:01:30</td></tr>\n",
       "<tr><td>Running for: </td><td>00:05:13.92        </td></tr>\n",
       "<tr><td>Memory:      </td><td>14.7/31.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 9.0/16 CPUs, 0/1 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_othello_dafdb_00000</td><td>RUNNING </td><td>127.0.0.1:21564</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         293.949</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">-0.976601</td><td style=\"text-align: right;\">            -0.90625</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">           1.63051</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 12:56:16,232\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=21564)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=21564)\u001b[0m 2023-08-10 12:56:20,671\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=21564)\u001b[0m 2023-08-10 12:56:20,671\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=17936)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=8408)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17936)\u001b[0m 2023-08-10 12:56:26,574\tWARNING multi_agent_env.py:274 -- observation_space_sample() of <OthelloEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17936)\u001b[0m 2023-08-10 12:56:26,574\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <OthelloEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17936)\u001b[0m 2023-08-10 12:56:26,574\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <OthelloEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17936)\u001b[0m 2023-08-10 12:56:26,575\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <OthelloEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17936)\u001b[0m 2023-08-10 12:56:26,575\tWARNING multi_agent_env.py:237 -- action_space_sample() of <OthelloEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17936)\u001b[0m 2023-08-10 12:56:26,575\tWARNING multi_agent_env.py:199 -- action_space_contains() of <OthelloEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17936)\u001b[0m 2023-08-10 12:56:26,575\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <OthelloEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17936)\u001b[0m 2023-08-10 12:56:26,578\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17936)\u001b[0m 2023-08-10 12:56:26,589\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17936)\u001b[0m 2023-08-10 12:56:26,589\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17936)\u001b[0m 2023-08-10 12:56:26,589\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17936)\u001b[0m 2023-08-10 12:56:26,590\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17936)\u001b[0m 2023-08-10 12:56:26,602\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=21564)\u001b[0m Install gputil for GPU system monitoring.\n",
      "2023-08-10 13:01:19,188\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: [WinError 32] Failed copying 'C:/Users/yoshi/ray_results/PPO/basic-variant-state-2023-08-10_12-07-41.json' to 'c:/Users/yoshi/ray_results/PPO/basic-variant-state-2023-08-10_12-07-41.json'. Detail: [Windows error 32] �v���Z�X�̓t�@�C���ɃA�N�Z�X�ł��܂���B�ʂ̃v���Z�X���g�p���ł��B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray import air\n",
    "from ray import tune\n",
    "\n",
    "results = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config,\n",
    "    run_config=air.RunConfig(\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            checkpoint_at_end=True,\n",
    "        )\n",
    "    ),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResultGrid<[\n",
       "  Result(\n",
       "    error='TuneError',\n",
       "    metrics={'trial_id': '79536_00000'},\n",
       "    path='c://\\\\Users\\\\yoshi\\\\ray_results\\\\PPO\\\\PPO_othello_79536_00000_0_2023-08-10_12-53-32',\n",
       "    checkpoint=None\n",
       "  )\n",
       "]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 2023-08-10 12:53:41,398\tERROR actor_manager.py:500 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=24508, ip=127.0.0.1, actor_id=0752922d0a6b44d53ffa963b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000017992C9F100>)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\Documents\\Repository\\rl-mini-app\\rl\\envs.py\", line 134, in step\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     assert (\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m AssertionError: Only one agent can take action at a time. {}\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=24508, ip=127.0.0.1, actor_id=0752922d0a6b44d53ffa963b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000017992C9F100>)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     check_multiagent_environments(env)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 368, in check_multiagent_environments\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m ValueError: Your environment (<OthelloEnv instance>) does not abide to the new gymnasium-style API!\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In particular, the `step()` method seems to be faulty.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `reset()` method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    setting).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    per-agent dict).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=24508, ip=127.0.0.1, actor_id=0752922d0a6b44d53ffa963b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000017992C9F100>)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 404, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     check_env(self.env, self.config)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 96, in check_env\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m ValueError: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 363, in check_multiagent_environments\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     results = env.step(sampled_action)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\Documents\\Repository\\rl-mini-app\\rl\\envs.py\", line 134, in step\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     assert (\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m AssertionError: Only one agent can take action at a time. {}\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=24508, ip=127.0.0.1, actor_id=0752922d0a6b44d53ffa963b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000017992C9F100>)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     check_multiagent_environments(env)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 368, in check_multiagent_environments\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m ValueError: Your environment (<OthelloEnv instance>) does not abide to the new gymnasium-style API!\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In particular, the `step()` method seems to be faulty.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `reset()` method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    setting).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    per-agent dict).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 2023-08-10 12:53:41,398\tERROR actor_manager.py:500 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=10336, ip=127.0.0.1, actor_id=f8de145a93cd22a0c59c458f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001BE92C9F130>)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\Documents\\Repository\\rl-mini-app\\rl\\envs.py\", line 134, in step\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     assert (\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m AssertionError: Only one agent can take action at a time. {}\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=10336, ip=127.0.0.1, actor_id=f8de145a93cd22a0c59c458f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001BE92C9F130>)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     check_multiagent_environments(env)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 368, in check_multiagent_environments\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m ValueError: Your environment (<OthelloEnv instance>) does not abide to the new gymnasium-style API!\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In particular, the `step()` method seems to be faulty.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `reset()` method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    setting).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    per-agent dict).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=10336, ip=127.0.0.1, actor_id=f8de145a93cd22a0c59c458f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001BE92C9F130>)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 404, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     check_env(self.env, self.config)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 96, in check_env\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m ValueError: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 363, in check_multiagent_environments\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     results = env.step(sampled_action)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\Documents\\Repository\\rl-mini-app\\rl\\envs.py\", line 134, in step\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     assert (\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m AssertionError: Only one agent can take action at a time. {}\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=10336, ip=127.0.0.1, actor_id=f8de145a93cd22a0c59c458f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001BE92C9F130>)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     check_multiagent_environments(env)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 368, in check_multiagent_environments\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m ValueError: Your environment (<OthelloEnv instance>) does not abide to the new gymnasium-style API!\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In particular, the `step()` method seems to be faulty.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `reset()` method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    setting).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    per-agent dict).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=23440, ip=127.0.0.1, actor_id=59744cde9fe348599de3dd4c01000000, repr=PPO)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 227, in _setup\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     self.add_workers(\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 593, in add_workers\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     raise result.get()\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 481, in __fetch_result\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     result = ray.get(r)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\_private\\worker.py\", line 2495, in get\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     raise value\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=24508, ip=127.0.0.1, actor_id=0752922d0a6b44d53ffa963b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000017992C9F100>)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\Documents\\Repository\\rl-mini-app\\rl\\envs.py\", line 134, in step\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     assert (\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m AssertionError: Only one agent can take action at a time. {}\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=24508, ip=127.0.0.1, actor_id=0752922d0a6b44d53ffa963b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000017992C9F100>)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     check_multiagent_environments(env)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 368, in check_multiagent_environments\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m ValueError: Your environment (<OthelloEnv instance>) does not abide to the new gymnasium-style API!\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In particular, the `step()` method seems to be faulty.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `reset()` method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    setting).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    per-agent dict).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=24508, ip=127.0.0.1, actor_id=0752922d0a6b44d53ffa963b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000017992C9F100>)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 404, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     check_env(self.env, self.config)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 96, in check_env\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m ValueError: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 363, in check_multiagent_environments\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     results = env.step(sampled_action)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\Documents\\Repository\\rl-mini-app\\rl\\envs.py\", line 134, in step\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     assert (\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m AssertionError: Only one agent can take action at a time. {}\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=24508, ip=127.0.0.1, actor_id=0752922d0a6b44d53ffa963b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000017992C9F100>)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     check_multiagent_environments(env)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 368, in check_multiagent_environments\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m ValueError: Your environment (<OthelloEnv instance>) does not abide to the new gymnasium-style API!\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In particular, the `step()` method seems to be faulty.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `reset()` method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    setting).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    per-agent dict).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \u001b[36mray::PPO.__init__()\u001b[39m (pid=23440, ip=127.0.0.1, actor_id=59744cde9fe348599de3dd4c01000000, repr=PPO)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 517, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 169, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 639, in setup\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 179, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     raise e.args[0].args[2]\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m ValueError: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 363, in check_multiagent_environments\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     results = env.step(sampled_action)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\Documents\\Repository\\rl-mini-app\\rl\\envs.py\", line 134, in step\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     assert (\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m AssertionError: Only one agent can take action at a time. {}\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \u001b[36mray::PPO.__init__()\u001b[39m (pid=23440, ip=127.0.0.1, actor_id=59744cde9fe348599de3dd4c01000000, repr=PPO)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     check_multiagent_environments(env)\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m   File \"c:\\Users\\yoshi\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 368, in check_multiagent_environments\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m ValueError: Your environment (<OthelloEnv instance>) does not abide to the new gymnasium-style API!\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In particular, the `step()` method seems to be faulty.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    `reset()` method.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    setting).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    per-agent dict).\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=23440)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10336)\u001b[0m 2023-08-10 12:53:41,388\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <OthelloEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10336)\u001b[0m 2023-08-10 12:53:41,388\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <OthelloEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10336)\u001b[0m 2023-08-10 12:53:41,389\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <OthelloEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10336)\u001b[0m 2023-08-10 12:53:41,389\tWARNING multi_agent_env.py:237 -- action_space_sample() of <OthelloEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10336)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=10336, ip=127.0.0.1, actor_id=f8de145a93cd22a0c59c458f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001BE92C9F130>)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = results.get_best_result().checkpoint\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      ".|.|.|X|O|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False 0.046875\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      ".|.|.|O|O|.|.|.\n",
      ".|.|.|O|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False -0.0\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False 0.046875\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|O|.|.\n",
      ".|.|.|O|O|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False -0.0\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|O|.|.\n",
      ".|.|.|O|O|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      ".|.|.|X|X|.|.|.\n",
      ".|.|X|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False 0.046875\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|O|.|.\n",
      ".|.|.|O|O|.|.|.\n",
      ".|.|.|O|O|O|.|.\n",
      ".|.|.|X|X|.|.|.\n",
      ".|.|X|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False -0.0\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|O|.|.\n",
      ".|.|.|O|O|.|X|.\n",
      ".|.|.|O|O|X|.|.\n",
      ".|.|.|X|X|.|.|.\n",
      ".|.|X|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False 0.046875\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|O|.|.\n",
      ".|.|.|O|O|.|O|.\n",
      ".|.|.|O|O|X|.|O\n",
      ".|.|.|X|X|.|.|.\n",
      ".|.|X|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False 0.03125\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|O|.|.\n",
      ".|.|.|O|O|.|O|.\n",
      ".|.|X|X|X|X|.|O\n",
      ".|.|.|X|X|.|.|.\n",
      ".|.|X|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False 0.015625\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|O|.|.\n",
      ".|.|.|O|O|.|O|.\n",
      ".|.|O|X|X|X|.|O\n",
      ".|O|.|X|X|.|.|.\n",
      ".|.|X|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False 0.03125\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|X|O|.|.\n",
      ".|.|.|O|X|.|O|.\n",
      ".|.|O|X|X|X|.|O\n",
      ".|O|.|X|X|.|.|.\n",
      ".|.|X|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False 0.015625\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|X|O|.|.\n",
      ".|.|.|O|X|.|O|.\n",
      ".|.|O|X|X|X|.|O\n",
      ".|O|.|O|X|.|.|.\n",
      ".|.|X|.|O|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False 0.03125\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|X|O|.|.\n",
      ".|.|X|X|X|.|O|.\n",
      ".|.|O|X|X|X|.|O\n",
      ".|O|.|O|X|.|.|.\n",
      ".|.|X|.|O|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False 0.109375\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|O|.|.|.\n",
      ".|.|.|.|O|O|.|.\n",
      ".|.|X|X|O|.|O|.\n",
      ".|.|O|X|O|X|.|O\n",
      ".|O|.|O|O|.|.|.\n",
      ".|.|X|.|O|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False -0.03125\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|O|.|X|.\n",
      ".|.|.|.|O|X|.|.\n",
      ".|.|X|X|X|.|O|.\n",
      ".|.|O|X|O|X|.|O\n",
      ".|O|.|O|O|.|.|.\n",
      ".|.|X|.|O|.|.|.\n",
      ".|.|.|.|.|.|.|.\n",
      "\n",
      "False False 0.078125\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|O|.|X|.\n",
      ".|.|.|.|O|X|.|.\n",
      ".|.|X|X|X|.|O|.\n",
      ".|.|O|X|O|X|.|O\n",
      ".|O|.|O|O|.|.|.\n",
      ".|.|O|.|O|.|.|.\n",
      ".|.|.|O|.|.|.|.\n",
      "\n",
      "False False -0.03125\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|O|.|X|.\n",
      ".|.|.|.|O|X|.|.\n",
      ".|.|X|X|X|.|O|.\n",
      ".|.|O|X|X|X|.|O\n",
      ".|O|.|O|O|X|.|.\n",
      ".|.|O|.|O|.|.|.\n",
      ".|.|.|O|.|.|.|.\n",
      "\n",
      "False False 0.109375\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|O|.|X|.\n",
      ".|.|.|O|O|X|.|.\n",
      ".|.|X|O|X|.|O|.\n",
      ".|.|O|O|X|X|.|O\n",
      ".|O|.|O|O|X|.|.\n",
      ".|.|O|.|O|.|.|.\n",
      ".|.|.|O|.|.|.|.\n",
      "\n",
      "False False -0.03125\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|O|.|X|.\n",
      ".|.|.|O|O|X|.|.\n",
      ".|.|X|O|X|.|O|.\n",
      ".|.|O|O|X|X|.|O\n",
      ".|O|.|O|X|X|.|.\n",
      ".|.|O|.|X|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      "\n",
      "False False 0.078125\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|O|.|X|.\n",
      ".|.|O|O|O|X|.|.\n",
      ".|.|O|O|X|.|O|.\n",
      ".|.|O|O|X|X|.|O\n",
      ".|O|.|O|X|X|.|.\n",
      ".|.|O|.|X|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      "\n",
      "False False -0.03125\n",
      ".|.|.|.|.|.|.|.\n",
      ".|.|.|.|O|.|X|.\n",
      ".|.|O|O|O|X|.|X\n",
      ".|.|O|O|X|.|X|.\n",
      ".|.|O|O|X|X|.|O\n",
      ".|O|.|O|X|X|.|.\n",
      ".|.|O|.|X|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      "\n",
      "False False 0.140625\n",
      ".|.|.|.|.|.|.|O\n",
      ".|.|.|.|O|.|O|.\n",
      ".|.|O|O|O|O|.|X\n",
      ".|.|O|O|O|.|X|.\n",
      ".|.|O|O|X|X|.|O\n",
      ".|O|.|O|X|X|.|.\n",
      ".|.|O|.|X|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      "\n",
      "False False -0.0625\n",
      ".|.|.|.|.|.|.|O\n",
      ".|.|X|.|O|.|O|.\n",
      ".|.|O|X|O|O|.|X\n",
      ".|.|O|O|X|.|X|.\n",
      ".|.|O|O|X|X|.|O\n",
      ".|O|.|O|X|X|.|.\n",
      ".|.|O|.|X|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      "\n",
      "False False 0.109375\n",
      ".|.|O|.|.|.|.|O\n",
      ".|.|O|.|O|.|O|.\n",
      ".|.|O|X|O|O|.|X\n",
      ".|.|O|O|X|.|X|.\n",
      ".|.|O|O|X|X|.|O\n",
      ".|O|.|O|X|X|.|.\n",
      ".|.|O|.|X|.|.|.\n",
      ".|.|.|O|X|.|.|.\n",
      "\n",
      "False False -0.03125\n",
      ".|.|O|.|.|.|.|O\n",
      ".|.|O|.|O|.|O|.\n",
      ".|.|O|X|O|O|.|X\n",
      ".|.|O|O|X|.|X|.\n",
      ".|.|O|O|X|X|.|O\n",
      ".|O|.|X|X|X|.|.\n",
      ".|.|X|.|X|.|.|.\n",
      ".|X|.|O|X|.|.|.\n",
      "\n",
      "False False 0.140625\n",
      ".|.|O|.|.|.|.|O\n",
      ".|.|O|.|O|.|O|.\n",
      ".|.|O|O|O|O|.|X\n",
      ".|.|O|O|O|.|X|.\n",
      ".|.|O|O|X|O|.|O\n",
      ".|O|.|X|X|X|O|.\n",
      ".|.|X|.|X|.|.|.\n",
      ".|X|.|O|X|.|.|.\n",
      "\n",
      "False False -0.09375\n",
      ".|.|O|.|.|.|.|O\n",
      ".|.|O|.|O|.|O|.\n",
      ".|.|O|O|O|O|.|X\n",
      ".|.|O|O|O|.|X|.\n",
      ".|.|O|O|X|O|.|O\n",
      ".|O|.|X|X|X|O|.\n",
      ".|.|X|.|X|.|.|.\n",
      ".|X|X|X|X|.|.|.\n",
      "\n",
      "False False 0.203125\n",
      ".|.|O|.|.|.|.|O\n",
      ".|.|O|.|O|.|O|.\n",
      ".|.|O|O|O|O|.|X\n",
      ".|.|O|O|O|.|X|.\n",
      ".|.|O|O|X|O|.|O\n",
      ".|O|O|O|O|O|O|.\n",
      ".|.|X|.|X|.|.|.\n",
      ".|X|X|X|X|.|.|.\n",
      "\n",
      "False False -0.125\n",
      ".|.|O|.|.|.|.|O\n",
      ".|X|O|.|O|.|O|.\n",
      ".|.|X|O|O|O|.|X\n",
      ".|.|O|X|O|.|X|.\n",
      ".|.|O|O|X|O|.|O\n",
      ".|O|O|O|O|O|O|.\n",
      ".|.|X|.|X|.|.|.\n",
      ".|X|X|X|X|.|.|.\n",
      "\n",
      "False False 0.171875\n",
      ".|.|O|.|.|.|.|O\n",
      ".|X|O|.|O|.|O|.\n",
      ".|.|X|O|O|O|.|X\n",
      ".|.|O|X|O|.|X|.\n",
      ".|.|O|O|X|O|.|O\n",
      ".|O|O|O|O|O|O|.\n",
      ".|.|X|.|O|.|.|.\n",
      ".|X|X|X|X|O|.|.\n",
      "\n",
      "False False -0.09375\n",
      ".|.|O|.|.|.|.|O\n",
      ".|X|O|.|O|.|O|.\n",
      ".|.|X|O|O|O|.|X\n",
      ".|.|O|X|O|.|X|.\n",
      ".|.|X|O|X|O|.|O\n",
      ".|X|O|O|O|O|O|.\n",
      "X|.|X|.|O|.|.|.\n",
      ".|X|X|X|X|O|.|.\n",
      "\n",
      "False False 0.234375\n",
      ".|.|O|.|.|.|.|O\n",
      ".|X|O|.|O|.|O|.\n",
      ".|.|X|O|O|O|.|X\n",
      ".|.|O|X|O|.|X|.\n",
      ".|.|X|O|X|O|.|O\n",
      ".|X|O|O|O|O|O|.\n",
      "X|.|X|.|O|.|.|.\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False -0.1875\n",
      ".|.|O|.|.|.|.|O\n",
      ".|X|O|.|O|.|O|.\n",
      ".|.|X|O|O|O|.|X\n",
      ".|.|O|X|X|X|X|.\n",
      ".|.|X|O|X|O|.|O\n",
      ".|X|O|O|O|O|O|.\n",
      "X|.|X|.|O|.|.|.\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False 0.234375\n",
      ".|.|O|.|.|.|.|O\n",
      ".|X|O|.|O|.|O|.\n",
      ".|.|X|O|O|O|.|X\n",
      ".|.|O|X|X|O|X|.\n",
      ".|.|X|O|X|O|O|O\n",
      ".|X|O|O|O|O|O|.\n",
      "X|.|X|.|O|.|.|.\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False -0.1875\n",
      ".|.|O|.|.|.|.|O\n",
      ".|X|O|.|O|.|O|.\n",
      ".|.|X|O|O|O|.|X\n",
      ".|X|X|X|X|O|X|.\n",
      ".|.|X|O|X|O|O|O\n",
      ".|X|O|O|O|O|O|.\n",
      "X|.|X|.|O|.|.|.\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False 0.328125\n",
      "O|.|O|.|.|.|.|O\n",
      ".|O|O|.|O|.|O|.\n",
      ".|.|O|O|O|O|.|X\n",
      ".|X|X|O|X|O|X|.\n",
      ".|.|X|O|O|O|O|O\n",
      ".|X|O|O|O|O|O|.\n",
      "X|.|X|.|O|.|.|.\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False -0.25\n",
      "O|.|O|.|.|.|.|O\n",
      ".|O|O|.|O|.|O|.\n",
      ".|.|O|O|O|O|.|X\n",
      ".|X|X|O|X|O|X|.\n",
      ".|.|X|O|O|X|O|O\n",
      ".|X|O|O|X|O|O|.\n",
      "X|.|X|X|O|.|.|.\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False 0.328125\n",
      "O|.|O|.|.|.|.|O\n",
      ".|O|O|.|O|.|O|.\n",
      ".|.|O|O|O|O|.|X\n",
      ".|X|X|O|X|O|X|.\n",
      ".|.|X|O|O|X|O|O\n",
      ".|X|O|O|X|O|O|.\n",
      "X|O|O|O|O|.|.|.\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False -0.25\n",
      "O|.|O|.|.|.|.|O\n",
      ".|O|O|.|O|.|O|.\n",
      ".|.|O|O|O|O|.|X\n",
      ".|X|X|O|X|O|X|.\n",
      ".|.|X|O|O|X|O|O\n",
      ".|X|O|O|X|X|X|X\n",
      "X|O|O|O|O|.|.|.\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False 0.296875\n",
      "O|.|O|.|.|.|.|O\n",
      ".|O|O|.|O|.|O|.\n",
      ".|.|O|O|O|O|O|X\n",
      ".|X|X|O|X|O|O|.\n",
      ".|.|X|O|O|X|O|O\n",
      ".|X|O|O|X|X|X|X\n",
      "X|O|O|O|O|.|.|.\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False -0.15625\n",
      "O|.|O|.|.|.|.|O\n",
      ".|O|O|.|O|.|O|.\n",
      ".|.|O|O|O|O|O|X\n",
      ".|X|X|O|X|X|X|X\n",
      ".|.|X|O|O|X|X|X\n",
      ".|X|O|O|X|X|X|X\n",
      "X|O|O|O|O|.|.|.\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False 0.265625\n",
      "O|.|O|.|.|.|.|O\n",
      ".|O|O|.|O|.|O|.\n",
      ".|.|O|O|O|O|O|X\n",
      ".|X|O|O|X|X|X|X\n",
      ".|O|O|O|O|X|X|X\n",
      ".|O|O|O|X|X|X|X\n",
      "X|O|O|O|O|.|.|.\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False -0.125\n",
      "O|.|O|.|.|.|.|O\n",
      ".|O|O|.|O|.|O|.\n",
      ".|.|O|O|O|O|O|X\n",
      ".|X|O|O|X|X|X|X\n",
      "X|X|X|X|X|X|X|X\n",
      ".|O|O|O|X|X|X|X\n",
      "X|O|O|O|O|.|.|.\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False 0.234375\n",
      "O|.|O|.|.|.|.|O\n",
      ".|O|O|.|O|.|O|.\n",
      ".|.|O|O|O|O|O|X\n",
      ".|X|O|O|O|X|X|X\n",
      "X|X|X|X|X|O|X|X\n",
      ".|O|O|O|X|X|O|X\n",
      "X|O|O|O|O|.|.|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False -0.09375\n",
      "O|.|O|.|.|X|.|O\n",
      ".|O|O|.|X|.|X|.\n",
      ".|.|O|X|O|O|O|X\n",
      ".|X|X|O|O|X|X|X\n",
      "X|X|X|X|X|O|X|X\n",
      ".|O|O|O|X|X|O|X\n",
      "X|O|O|O|O|.|.|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False 0.171875\n",
      "O|.|O|.|.|X|.|O\n",
      ".|O|O|.|X|.|X|.\n",
      ".|O|O|X|O|O|O|X\n",
      ".|O|X|O|O|X|X|X\n",
      "X|O|X|X|X|O|X|X\n",
      ".|O|O|O|X|X|O|X\n",
      "X|O|O|O|O|.|.|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False -0.0625\n",
      "O|.|O|.|.|X|.|O\n",
      ".|O|O|.|X|.|X|.\n",
      "X|X|X|X|O|O|O|X\n",
      ".|X|X|O|O|X|X|X\n",
      "X|O|X|X|X|O|X|X\n",
      ".|O|O|O|X|X|O|X\n",
      "X|O|O|O|O|.|.|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False 0.203125\n",
      "O|.|O|.|.|X|.|O\n",
      ".|O|O|.|X|.|X|O\n",
      "X|X|X|X|O|O|O|O\n",
      ".|X|X|O|O|X|X|O\n",
      "X|O|X|X|X|O|X|O\n",
      ".|O|O|O|X|X|O|O\n",
      "X|O|O|O|O|.|.|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False -0.09375\n",
      "O|.|O|.|.|X|.|O\n",
      ".|O|O|.|X|X|X|O\n",
      "X|X|X|X|X|X|O|O\n",
      ".|X|X|X|O|X|X|O\n",
      "X|O|X|X|X|O|X|O\n",
      ".|O|O|O|X|X|O|O\n",
      "X|O|O|O|O|.|.|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False 0.234375\n",
      "O|.|O|.|.|X|.|O\n",
      ".|O|O|.|X|X|X|O\n",
      "X|O|X|X|X|X|O|O\n",
      "O|O|O|O|O|X|X|O\n",
      "X|O|X|X|X|O|X|O\n",
      ".|O|O|O|X|X|O|O\n",
      "X|O|O|O|O|.|.|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False -0.15625\n",
      "O|.|O|.|.|X|.|O\n",
      "X|O|O|.|X|X|X|O\n",
      "X|X|X|X|X|X|O|O\n",
      "O|O|X|O|O|X|X|O\n",
      "X|O|X|X|X|O|X|O\n",
      ".|O|O|O|X|X|O|O\n",
      "X|O|O|O|O|.|.|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False 0.265625\n",
      "O|.|O|O|.|X|.|O\n",
      "X|O|O|.|O|X|X|O\n",
      "X|X|X|X|X|O|O|O\n",
      "O|O|X|O|O|X|O|O\n",
      "X|O|X|X|X|O|X|O\n",
      ".|O|O|O|X|X|O|O\n",
      "X|O|O|O|O|.|.|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False -0.15625\n",
      "O|.|O|O|.|X|.|O\n",
      "X|X|X|X|X|X|X|O\n",
      "X|X|X|X|X|O|O|O\n",
      "O|O|X|O|O|X|O|O\n",
      "X|O|X|X|X|O|X|O\n",
      ".|O|O|O|X|X|O|O\n",
      "X|O|O|O|O|.|.|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False 0.265625\n",
      "O|.|O|O|.|X|O|O\n",
      "X|X|X|X|X|O|O|O\n",
      "X|X|X|X|O|O|O|O\n",
      "O|O|X|O|O|X|O|O\n",
      "X|O|X|X|X|O|X|O\n",
      ".|O|O|O|X|X|O|O\n",
      "X|O|O|O|O|.|.|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False -0.125\n",
      "O|.|O|O|.|X|O|O\n",
      "X|X|X|X|X|O|O|O\n",
      "X|X|X|X|O|O|O|O\n",
      "O|O|X|O|O|X|O|O\n",
      "X|O|X|X|X|O|X|O\n",
      ".|O|O|O|X|X|O|O\n",
      "X|X|X|X|X|X|.|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False 0.265625\n",
      "O|.|O|O|O|O|O|O\n",
      "X|X|X|O|O|O|O|O\n",
      "X|X|O|X|O|O|O|O\n",
      "O|O|X|O|O|X|O|O\n",
      "X|O|X|X|X|O|X|O\n",
      ".|O|O|O|X|X|O|O\n",
      "X|X|X|X|X|X|.|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False -0.21875\n",
      "O|.|O|O|O|O|O|O\n",
      "X|X|X|O|O|O|O|O\n",
      "X|X|O|X|O|O|O|O\n",
      "O|O|X|O|O|X|O|O\n",
      "X|O|X|X|X|O|X|O\n",
      ".|O|O|O|X|X|X|O\n",
      "X|X|X|X|X|X|X|O\n",
      "O|O|O|O|O|O|.|.\n",
      "\n",
      "False False 0.328125\n",
      "O|.|O|O|O|O|O|O\n",
      "X|X|X|O|O|O|O|O\n",
      "X|X|O|X|O|O|O|O\n",
      "O|O|X|O|O|X|O|O\n",
      "X|O|X|X|O|O|X|O\n",
      ".|O|O|O|X|O|X|O\n",
      "X|X|X|X|X|X|O|O\n",
      "O|O|O|O|O|O|.|O\n",
      "\n",
      "False False -0.1875\n",
      "O|.|O|O|O|O|O|O\n",
      "X|X|X|O|O|O|O|O\n",
      "X|X|O|X|O|O|O|O\n",
      "O|O|X|O|O|X|O|O\n",
      "X|X|X|X|O|O|X|O\n",
      "X|X|X|X|X|O|X|O\n",
      "X|X|X|X|X|X|O|O\n",
      "O|O|O|O|O|O|.|O\n",
      "\n",
      "False False 0.328125\n",
      "O|O|O|O|O|O|O|O\n",
      "X|O|O|O|O|O|O|O\n",
      "X|O|O|O|O|O|O|O\n",
      "O|O|X|O|O|X|O|O\n",
      "X|X|X|X|O|O|X|O\n",
      "X|X|X|X|X|O|X|O\n",
      "X|X|X|X|X|X|O|O\n",
      "O|O|O|O|O|O|.|O\n",
      "\n",
      "True False -0.28125\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# multiagent environment for othello\n",
    "env = OthelloEnv({})\n",
    "obs, _ = env.reset()\n",
    "terminated = False\n",
    "current_player = \"agent_1\"\n",
    "while not terminated:\n",
    "    env.render()\n",
    "    # action = env.action_space.sample()\n",
    "    valid_actions = env.get_valid_moves(current_player)\n",
    "    if len(valid_actions) > 0:\n",
    "        action = random.choice(valid_actions)\n",
    "    else:\n",
    "        action = 64\n",
    "    action = {current_player: action}\n",
    "    obs, reward, terminated, truncated, _= env.step(action)\n",
    "    obs = obs[\"agent_1\" if current_player == \"agent_2\" else \"agent_2\"]\n",
    "    reward = reward[current_player]\n",
    "    terminated = terminated[current_player]\n",
    "    truncated = truncated[current_player]\n",
    "    current_player = env.current_player\n",
    "    print(terminated, truncated, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O|O|O|O|O|O|O|O\n",
      "X|O|O|O|O|O|O|O\n",
      "X|O|O|O|O|O|O|O\n",
      "O|O|X|O|O|X|O|O\n",
      "X|X|X|X|O|O|X|O\n",
      "X|X|X|X|X|O|X|O\n",
      "X|X|X|X|X|X|X|O\n",
      "O|O|O|O|O|O|X|O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
