{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 14:44:16,127\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms import ppo\n",
    "from envs import TicTacToeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 8, Reward: 0, Terminated: False, Info: {}\n",
      "Observation: [<Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.PLAYER_2: 2>, <Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.PLAYER_1: 1>]\n",
      "State: [<Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.PLAYER_2: 2>, <Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.PLAYER_1: 1>]\n",
      "\n",
      "Action: 6, Reward: 0, Terminated: False, Info: {}\n",
      "Observation: [<Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.PLAYER_2: 2>, <Mark.EMPTY: 0>, <Mark.PLAYER_2: 2>, <Mark.PLAYER_1: 1>, <Mark.EMPTY: 0>, <Mark.PLAYER_1: 1>]\n",
      "State: [<Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.PLAYER_2: 2>, <Mark.EMPTY: 0>, <Mark.PLAYER_2: 2>, <Mark.PLAYER_1: 1>, <Mark.EMPTY: 0>, <Mark.PLAYER_1: 1>]\n",
      "\n",
      "Action: 5, Reward: -1, Terminated: True, Info: {}\n",
      "Observation: [<Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.PLAYER_2: 2>, <Mark.EMPTY: 0>, <Mark.PLAYER_2: 2>, <Mark.PLAYER_1: 1>, <Mark.EMPTY: 0>, <Mark.PLAYER_1: 1>]\n",
      "State: [<Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.EMPTY: 0>, <Mark.PLAYER_2: 2>, <Mark.EMPTY: 0>, <Mark.PLAYER_2: 2>, <Mark.PLAYER_1: 1>, <Mark.EMPTY: 0>, <Mark.PLAYER_1: 1>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToeEnv(env_config={\"train\": True})\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, _, info = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}, Terminated: {terminated}, Info: {info}\")\n",
    "    print(f\"Observation: {obs}\")\n",
    "    print(f\"State: {env.state}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(max_episodes=10):\n",
    "    config = ppo.PPOConfig().environment(env=TicTacToeEnv, env_config={\"train\": True})\n",
    "    config = config.rl_module(_enable_rl_module_api=False).training(_enable_learner_api=False)\n",
    "    algo = config.build()\n",
    "    \n",
    "    # tqdm pbar\n",
    "    with tqdm(total=max_episodes) as pbar:\n",
    "        for i in range(max_episodes):\n",
    "            result = algo.train()\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f\"episode_reward_mean: {result['episode_reward_mean']}\")\n",
    "\n",
    "    return algo\n",
    "\n",
    "\n",
    "def evaluate(algo):\n",
    "    env = TicTacToeEnv()\n",
    "    observation, info = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        # action = env.action_space.sample()\n",
    "        action = algo.compute_single_action(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        print(\"observation: \", observation)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 14:44:16,571\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-08-04 14:44:16,573\tWARNING algorithm_config.py:2548 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-08-04 14:44:16,577\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "c:\\Users\\lcglab\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "c:\\Users\\lcglab\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\lcglab\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\lcglab\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-08-04 14:44:22,371\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(pid=6676)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40388)\u001b[0m c:\\Users\\lcglab\\miniconda3\\envs\\rl\\lib\\site-packages\\gymnasium\\spaces\\box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40388)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40388)\u001b[0m 2023-08-04 14:44:32,585\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6676)\u001b[0m 2023-08-04 14:44:32,565\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6676)\u001b[0m 2023-08-04 14:44:32,580\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6676)\u001b[0m 2023-08-04 14:44:32,580\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6676)\u001b[0m 2023-08-04 14:44:32,588\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6676)\u001b[0m 2023-08-04 14:44:32,588\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6676)\u001b[0m 2023-08-04 14:44:32,589\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6676)\u001b[0m 2023-08-04 14:44:32,589\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6676)\u001b[0m 2023-08-04 14:44:32,589\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6676)\u001b[0m 2023-08-04 14:44:32,589\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6676)\u001b[0m 2023-08-04 14:44:32,589\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6676)\u001b[0m 2023-08-04 14:44:32,591\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "2023-08-04 14:44:32,669\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-08-04 14:44:32,680\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "2023-08-04 14:44:32,681\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "2023-08-04 14:44:32,707\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "2023-08-04 14:44:32,709\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "2023-08-04 14:44:32,711\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "2023-08-04 14:44:32,715\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-08-04 14:44:32,717\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "2023-08-04 14:44:32,721\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "2023-08-04 14:44:32,723\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-08-04 14:44:32,731\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "2023-08-04 14:44:32,750\tINFO trainable.py:172 -- Trainable.setup took 16.110 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-08-04 14:44:32,753\tWARNING util.py:68 -- Install gputil for GPU system monitoring.\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]2023-08-04 14:44:37,744\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "episode_reward_mean: 0.520723436322532: 100%|██████████| 100/100 [22:31<00:00, 13.51s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "algo = train(max_episodes=100)\n",
    "algo.export_policy_model(\"policy_model\", onnx=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"policy_model/model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.checker.check_model(onnx_model, full_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9)\n",
      "[[0 0 0 0 0 2 1 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[  1.5947901 ,   9.396042  ,   0.59201837,   1.4645842 ,\n",
       "           1.6824638 , -17.239763  ,  -7.9333267 ,   7.101857  ,\n",
       "           4.3303375 ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "env = TicTacToeEnv({})\n",
    "\n",
    "# obs = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "obs = np.zeros((1, 9)).astype(np.int32)\n",
    "state_ins = np.zeros((1,)).astype(np.float32)\n",
    "# \n",
    "obs, info = env.reset()\n",
    "obs = np.array(obs).astype(np.int32).reshape(1, -1)\n",
    "print(obs.shape)\n",
    "print(obs)\n",
    "ort_sess = ort.InferenceSession('policy_model/model.onnx', providers=['CPUExecutionProvider'])\n",
    "outputs = ort_sess.run(None, {'obs': obs, 'state_ins': state_ins})\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.5947901 ,   9.396042  ,   0.59201837,   1.4645842 ,\n",
       "          1.6824638 , -17.239763  ,  -7.9333267 ,   7.101857  ,\n",
       "          4.3303375 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPOTorchPolicy\n"
     ]
    }
   ],
   "source": [
    "policy = algo.get_policy()\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 2, (9,), int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.observation_space_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
